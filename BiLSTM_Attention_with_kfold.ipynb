{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "data preparation(by k_fold):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# constants:\n",
    "GESTURE_NUM = 13\n",
    "VCC = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13, 390)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load data\n",
    "ges_dfs = [[i] for i in range(GESTURE_NUM)]\n",
    "folder_path = \".\\\\gesture_data\\\\\"\n",
    "for i in range(GESTURE_NUM):\n",
    "    file_path = folder_path + f\"gesture{i+1}.csv\"\n",
    "    df = pd.read_csv(file_path)\n",
    "    zero_rows = (df == 0).all(axis=1)\n",
    "    zero_rows_indexes = list(zero_rows[zero_rows].index)\n",
    "    ges_df_list = []\n",
    "    ges_dfs[i].pop()\n",
    "    start = 0\n",
    "    for j in range(len(zero_rows_indexes)):\n",
    "        ges_df = df.iloc[start:zero_rows_indexes[j],:]\n",
    "        start = zero_rows_indexes[j] + 1\n",
    "        ges_df_list.append(ges_df)     \n",
    "    ges_dfs[i].extend(ges_df_list)\n",
    "\n",
    "len(ges_dfs),len(ges_dfs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14, 10)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ges_dfs[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get time-step in lstm\n",
    "time_step =zero_rows_indexes[1] - zero_rows_indexes[0] - 1 # remove all-zero row\n",
    "time_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_shuffle(setX:list,setY:list):\n",
    "    \"\"\"\n",
    "    shuffle setX and setY, but still keep setX and setY's mapping\n",
    "    return:\n",
    "       setX,setY\n",
    "    \"\"\"\n",
    "    import random\n",
    "    ges_set = list(zip(setY,setX))\n",
    "    keys = [i+1 for i in range(len(ges_set))]\n",
    "    ges_set_dict = dict(zip(keys,ges_set))\n",
    "    random.shuffle(keys)\n",
    "    setX = []\n",
    "    setY = []\n",
    "    for i in range(len(keys)):\n",
    "        ges_sample = ges_set_dict[keys[i]]\n",
    "        setX.append(ges_sample[1])\n",
    "        setY.append(ges_sample[0])\n",
    "    return setX,setY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get training set, CV set and testing set:\n",
    "ges_trainingX = []\n",
    "ges_trainingY = []\n",
    "# ges_validationX = []\n",
    "# ges_validationY = []\n",
    "ges_testingX = []\n",
    "ges_testingY = []\n",
    "\n",
    "\n",
    "for i in range(GESTURE_NUM):\n",
    "    test_split = round(len(ges_dfs[i]) * 0.10) \n",
    "    ges_trainingX.extend(ges_dfs[i][:len(ges_dfs[i])-test_split]) # training set\n",
    "    ges_testingX.extend(ges_dfs[i][len(ges_dfs[i])-test_split:]) # test set\n",
    "\n",
    "    ges_trainingY.extend([i+1]*(len(ges_dfs[i])-test_split))\n",
    "    ges_testingY.extend([i+1]*test_split)\n",
    "\n",
    "# shuffle training and testing set\n",
    "ges_trainingX,ges_trainingY = set_shuffle(ges_trainingX,ges_trainingY)\n",
    "ges_testingX,ges_testingY = set_shuffle(ges_testingX,ges_testingY)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainX shape-- ( 4563 , (14, 10) )\n",
      "testX shape-- ( 507 , (14, 10) )\n"
     ]
    }
   ],
   "source": [
    "# shape of all sets:\n",
    "print(\"trainX shape--\",\"(\",len(ges_trainingX),\",\",ges_trainingX[0].shape,\")\")\n",
    "print(\"testX shape--\",\"(\",len(ges_testingX),\",\",ges_testingX[0].shape,\")\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concat all training data in X, and do normalization:\n",
    "ges_trainingX_df = pd.concat(ges_trainingX)\n",
    "ges_testingX_df = pd.concat(ges_testingX)\n",
    "# #用于后续性能优化：让analog pin口读数转为电压\n",
    "# ges_trainingX_df = transfrom_flex_raw(ges_trainingX_df)\n",
    "# ges_testingX_df = transfrom_flex_raw(ges_testingX_df)\n",
    "\n",
    "# transfrom flex sensor data to voltage:\n",
    "\n",
    "scaler = MinMaxScaler(feature_range=(0,1))\n",
    "ges_trainingX_scaled = scaler.fit_transform(ges_trainingX_df)\n",
    "ges_testingX_scaled = scaler.fit_transform(ges_testingX_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((63882, 10), (7098, 10))"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ges_trainingX_scaled.shape,ges_testingX_scaled.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def splitX(dataset,time_step):\n",
    "    dataX = []\n",
    "    for i in range(time_step,len(dataset)+time_step,time_step):\n",
    "        dataX.append(dataset[i-time_step:i,0:dataset.shape[1]]) \n",
    "    return np.array(dataX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_categorical_numpy(y, num_classes=None):\n",
    "    \"\"\"\n",
    "    将整数数组转换为 one-hot 编码的 NumPy 数组。\n",
    "    \n",
    "    参数:\n",
    "    - y: 一个包含整数标签的 1D NumPy 数组。\n",
    "    - num_classes: one-hot 编码的目标类别数。\n",
    "    \n",
    "    返回:\n",
    "    - one_hot: one-hot 编码的 2D NumPy 数组。\n",
    "    \"\"\"\n",
    "    if num_classes is None:\n",
    "        num_classes = np.max(y) + 1\n",
    "    \n",
    "    one_hot = np.zeros((y.shape[0], num_classes))\n",
    "    one_hot[np.arange(y.shape[0]), y] = 1\n",
    "    \n",
    "    return one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 用numpy写了原来keras中to_categorical的平替方法\n",
    "ges_trainingX = splitX(ges_trainingX_scaled,time_step)\n",
    "ges_trainingY = np.array(ges_trainingY) # generate an one-hot encoding for labels, this encoding is a dim=14 vector, where first element points to label=0(however, no label here =0)\n",
    "ges_testingX = splitX(ges_testingX_scaled,time_step)\n",
    "ges_testingY = np.array(ges_testingY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainX Shape--  (4563, 14, 10)\n",
      "trainY Shape--  (4563,)\n",
      "testX Shape--  (507, 14, 10)\n",
      "testY Shape--  (507,)\n"
     ]
    }
   ],
   "source": [
    "# view shapes\n",
    "print(\"trainX Shape-- \",ges_trainingX.shape)\n",
    "print(\"trainY Shape-- \",ges_trainingY.shape)\n",
    "print(\"testX Shape-- \",ges_testingX.shape)\n",
    "print(\"testY Shape-- \",ges_testingY.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "训练之前，将训练集，验证集和测试集封装成torch中的dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader,TensorDataset,SubsetRandomSampler\n",
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将 NumPy 数组转换为 PyTorch Tensor\n",
    "trainX_tensor = torch.tensor(ges_trainingX,dtype=torch.float32)\n",
    "trainY_tensor = torch.tensor(ges_trainingY,dtype=torch.float32)\n",
    "testX_tensor = torch.tensor(ges_testingX,dtype=torch.float32)\n",
    "testY_tensor = torch.tensor(ges_testingY,dtype=torch.float32)\n",
    "\n",
    "# 创建 TensorDataset\n",
    "train_dataset = TensorDataset(trainX_tensor, trainY_tensor)\n",
    "test_dataset = TensorDataset(testX_tensor, testY_tensor)\n",
    "\n",
    "# 创建 DataLoader\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)  # 测试集通常不打乱"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model training："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# constants\n",
    "num_layers = 1  #一层lstm\n",
    "num_directions = 2  #双向lstm\n",
    "lr = 1e-3 # 学习率\n",
    "batch_size = 16   \n",
    "epochs = 20\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "GESTURE_NUM = 13\n",
    "num_classses =GESTURE_NUM+1\n",
    "time_step = 14\n",
    "input_size = 10\n",
    "hidden_size = 50 # how many hidden layer on LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiLSTMModel(nn.Module):\n",
    "    def __init__(self, input_size,hidden_size, num_layers, num_directions, num_classes,dropout_prob):\n",
    "        super(BiLSTMModel, self).__init__()\n",
    "\n",
    "        self.input_size = input_size # 数据维度\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.num_directions = num_directions\n",
    "        \n",
    "        \n",
    "        self.lstm1 = nn.LSTM(input_size, hidden_size, \n",
    "                             num_layers = num_layers, bidirectional = (num_directions == 2))\n",
    "        self.lstm2 = nn.LSTM(hidden_size, hidden_size, \n",
    "                             num_layers = num_layers, bidirectional = (num_directions == 2))\n",
    "        self.dropout = nn.Dropout(dropout_prob)\n",
    "        self.attention_weights_layer = nn.Sequential(\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        self.liner = nn.Linear(hidden_size, num_classes)\n",
    "        self.act_func = nn.Softmax(dim=1)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        \"\"\"\n",
    "        x: input data with shape: [batch_size, time_step, embedding_size]\n",
    "        \"\"\"\n",
    "        #x [batch_size, time_step, embedding_size]\n",
    "        x = x.permute(1, 0, 2) #[time_step, batch_size, embedding_size]\n",
    "        #LSTM 期望将时间步放在最前面        \n",
    "        #由于数据集不一定是预先设置的batch_size的整数倍，所以用size(1)获取当前数据实际的batch\n",
    "        batch_size = x.size(1)\n",
    "\n",
    "        # LSTM最初的前向输出，即记忆单元C和隐状图H\n",
    "        h_0 = torch.randn(self.num_layers * self.num_directions, batch_size, self.hidden_size).to(device)\n",
    "        c_0 = torch.randn(self.num_layers * self.num_directions, batch_size, self.hidden_size).to(device)\n",
    "\n",
    "        # 第一层LSTM\n",
    "        #out[seq_len, batch, num_directions * hidden_size]。多层lstm，out只保存最后一层每个时间步t的输出h_t\n",
    "        #h_n, c_n [num_layers(1) * num_directions, batch, hidden_size]，h_n是全部\n",
    "        out, (h_n, c_n) = self.lstm1(x, (h_0, c_0))\n",
    "        # 第一层Dropout，\n",
    "        out = self.dropout(out)\n",
    "        #双向LSTM输出拆成前向和后向输出\n",
    "        #将双向lstm的输出拆分为前向输出和后向输出\n",
    "        (forward_out, backward_out) = torch.chunk(out, 2, dim = 2)\n",
    "        out = forward_out + backward_out  #[seq_len, batch, hidden_size]\n",
    "\n",
    "         # 第二层LSTM\n",
    "        out, (h_n, c_n) = self.lstm2(out, (h_n, c_n))\n",
    "        # 第二层Dropout\n",
    "        out = self.dropout(out)\n",
    "        \n",
    "        # 再次将双向LSTM的输出拆分为前向输出和后向输出，并求和\n",
    "        (forward_out, backward_out) = torch.chunk(out, 2, dim=2)\n",
    "        out = forward_out + backward_out  # [seq_len, batch, hidden_size]\n",
    "    \n",
    "        # 调整out的维度以符合后续处理的要求\n",
    "        out = out.permute(1, 0, 2)  # [batch, seq_len, hidden_size]\n",
    "        #print(\"LSTM out\",out)\n",
    "         #为了使用到lstm最后一个时间步时每层lstm的表达，用h_n生成attention的权重\n",
    "        h_n = h_n.permute(1, 0, 2)  #[batch, num_layers * num_directions,  hidden_size]\n",
    "        h_n = torch.sum(h_n, dim=1) #[batch, 1,  hidden_size]\n",
    "        h_n = h_n.squeeze(dim=1)  #[batch, hidden_size]\n",
    "        # 得到attention层的权重\n",
    "        attention_w = self.attention_weights_layer(h_n)  #[batch, hidden_size]\n",
    "        attention_w = attention_w.unsqueeze(dim=1) #[batch, 1, hidden_size]\n",
    "        attention_context = torch.bmm(attention_w, out.transpose(1, 2))  #[batch, 1, seq_len]\n",
    "        #print(\"attention_context\",x,sep='\\n')\n",
    "        softmax_w = F.softmax(attention_context, dim=-1)  #[batch, 1, seq_len], 用softmax对刚才的权重归一化,a-> a'\n",
    "        \n",
    "        x = torch.bmm(softmax_w, out)  #[batch, 1, hidden_size] 抽取sequence内的重要信息\n",
    "        x = x.squeeze(dim=1)  #[batch, hidden_size]\n",
    "        #print(\"after squeeze:\",x,sep='\\n')\n",
    "        x = self.liner(x)\n",
    "        #x = self.act_func(x)\n",
    "        return x\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, test_loader, loss_func):\n",
    "    model.eval()\n",
    "    loss_val = 0.0\n",
    "    corrects = 0.0\n",
    "    for datas, labels in test_loader:\n",
    "        datas = datas.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        preds = model(datas)\n",
    "        loss = loss_func(preds, labels.long())\n",
    "        loss_val += loss.item() * datas.size(0)\n",
    "        \n",
    "        #获取预测的最大概率出现的位置\n",
    "        preds = nn.Softmax(dim=1)(preds)\n",
    "        preds = torch.argmax(preds, dim=1)\n",
    "        #labels = torch.argmax(labels, dim=0)\n",
    "        corrects += torch.sum(preds == labels).item()\n",
    "    test_loss = loss_val / len(test_loader.dataset) # 计算整个测试集的总损失\n",
    "    test_acc = corrects / len(test_loader.dataset) # 计算整个测试集的总正确率\n",
    "    print(f\"Test Loss: {test_loss:.4f}, Test Acc: {test_acc:.4f}\")\n",
    "    return test_loss,test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, optimizer, loss_func, epochs,k_fold):\n",
    "    \"\"\"\n",
    "    define training process of upon model\n",
    "    return:\n",
    "    model: the model after training\n",
    "    train_losses: loss for each epoch in training\n",
    "    train_accss: accuracy for each epoch in training\n",
    "    CV_losses: loss for each epoch in validation\n",
    "    CV_accs: accuracy for each epoch in validation\n",
    "    \"\"\"\n",
    "    k = k_fold # k折交叉验证\n",
    "    best_val_acc_list = [0.0 for i in range(k)] # 跟踪每个折最佳验证准确率\n",
    "    best_models = [] # 记录每一折训练得到的最优model，然后根据正确率得到一个全局最优model\n",
    "    best_model_params = copy.deepcopy(model.state_dict()) # 创建当前模型参数的深拷贝\n",
    "    kf = KFold(n_splits=k)\n",
    "    average_val_acc = 0.0\n",
    "    train_loss_folds = []\n",
    "    train_acc_folds = []\n",
    "    CV_loss_folds = []\n",
    "    CV_acc_folds = []\n",
    "    for fold,(train_indexes,val_indexes) in enumerate(kf.split(train_loader.dataset)):\n",
    "        train_sampler = SubsetRandomSampler(train_indexes) # 告诉dataloader应该加载与len(train_indexes)数量相同，与train_indexes对应的样本\n",
    "        val_sampler = SubsetRandomSampler(val_indexes)\n",
    "        curr_train_loader = DataLoader(train_loader.dataset,batch_size=16,sampler=train_sampler)\n",
    "        val_loader = DataLoader(train_loader.dataset,batch_size=16,sampler=val_sampler) \n",
    "\n",
    "        # 记录训练损失和正确率，用于画图：\n",
    "        train_loss_epochs = []\n",
    "        train_acc_epochs = []\n",
    "        # 记录每次验证的损失和正确率，用于画图：\n",
    "        CV_loss_epochs = []\n",
    "        CV_acc_epochs = []\n",
    "        for epoch in range(epochs):\n",
    "            print(f\"Fold{fold+1}, epoch{epoch+1}:...\")\n",
    "            model.train() # 设置为训练模式\n",
    "            loss_val = 0.0\n",
    "            corrects = 0.0\n",
    "            for datas, labels in curr_train_loader:\n",
    "                # datas: (batch_size,input_size(14),features(10))\n",
    "                # labels: (batch_size,input_size(14))\n",
    "                datas = datas.to(device)\n",
    "                labels = labels.to(device)\n",
    "                \n",
    "                preds = model(datas) # 前向传播\n",
    "                #print(labels.long())\n",
    "                loss = loss_func(preds, labels.long()) # 计算损失,tensor大小是1\n",
    "                \n",
    "                optimizer.zero_grad() # 清除优化器梯度（来自于上一次反向传播）\n",
    "                loss.backward() # 反向传播, 计算模型参数梯度\n",
    "                optimizer.step() # 根据计算得到的梯度，使用优化器更新模型的参数。\n",
    "                \n",
    "                loss_val += loss.item() * datas.size(0) #获取loss，并乘以当前批次大小\n",
    "                \n",
    "                #获取预测的最大概率出现的位置\n",
    "                preds = nn.Softmax(dim=1)(preds)\n",
    "                preds = torch.argmax(preds, dim=1)\n",
    "                #labels = torch.argmax(labels, dim=0)\n",
    "                corrects += torch.sum(preds == labels).item()\n",
    "            train_loss = loss_val / len(curr_train_loader.dataset) # 计算整个模型的总损失\n",
    "            train_acc = corrects / len(curr_train_loader.dataset) # 计算整个模型的总正确率\n",
    "            print(f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}\")\n",
    "            train_loss_epochs.append(train_loss)\n",
    "            train_acc_epochs.append(train_acc)\n",
    "            # if(epoch % 2 == 0): 每个epoch都进行评估：\n",
    "            val_loss,val_acc = test(model, val_loader, loss_func)\n",
    "            if(best_val_acc_list[fold] < val_acc): #出现最优模型时，保存最优模型\n",
    "                best_val_acc_list[fold] = val_acc\n",
    "                best_model_params = copy.deepcopy(model.state_dict())\n",
    "            # 更新平均accuracy指标\n",
    "            average_val_acc+=val_acc\n",
    "            CV_loss_epochs.append(val_loss)\n",
    "            CV_acc_epochs.append(val_acc)\n",
    "        model.load_state_dict(best_model_params)\n",
    "        best_models.append(model)\n",
    "        train_loss_folds.append(train_loss_epochs)\n",
    "        train_acc_folds.append(train_acc_epochs)\n",
    "        CV_loss_folds.append(CV_loss_epochs)\n",
    "        CV_acc_folds.append(CV_acc_epochs)\n",
    "        print(len(CV_loss_folds),len(CV_acc_folds),sep=\",\")\n",
    "        \n",
    "    # 计算所有折的平均验证accuracy\n",
    "    average_val_acc = average_val_acc / (k*epochs)\n",
    "    print(f'Average Validation Accuracy: {average_val_acc:.4f}')\n",
    "    # 找到所有折中最好的model作为返回model，以及当前折的loss和acc记录返回\n",
    "    best_val_acc_index = np.argmax(np.array(best_val_acc_list))\n",
    "    print(best_val_acc_index)\n",
    "    model = best_models[best_val_acc_index]\n",
    "    train_losses = train_loss_folds[best_val_acc_index]\n",
    "    train_accs = train_acc_folds[best_val_acc_index]\n",
    "    CV_losses = CV_loss_folds[best_val_acc_index]\n",
    "    CV_accs = CV_acc_folds[best_val_acc_index]\n",
    "    return model,train_losses,train_accs,CV_losses,CV_accs\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5, 6, 7, 8, 9]\n",
      "9\n"
     ]
    }
   ],
   "source": [
    "test_a = np.arange(5)\n",
    "test_b = [i for i in range(5,10)]\n",
    "max_index = np.argmax(test_a)\n",
    "print(test_b)\n",
    "print(test_b[max_index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs=20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold1, epoch1:...\n",
      "Train Loss: 1.7829, Train Acc: 0.1457\n",
      "Test Loss: 0.3489, Test Acc: 0.0717\n",
      "Fold1, epoch2:...\n",
      "Train Loss: 1.2518, Train Acc: 0.3088\n",
      "Test Loss: 0.2821, Test Acc: 0.0936\n",
      "Fold1, epoch3:...\n",
      "Train Loss: 0.9647, Train Acc: 0.4181\n",
      "Test Loss: 0.2212, Test Acc: 0.1102\n",
      "Fold1, epoch4:...\n",
      "Train Loss: 0.8230, Train Acc: 0.4751\n",
      "Test Loss: 0.1848, Test Acc: 0.1289\n",
      "Fold1, epoch5:...\n",
      "Train Loss: 0.7281, Train Acc: 0.5119\n",
      "Test Loss: 0.1711, Test Acc: 0.1319\n",
      "Fold1, epoch6:...\n",
      "Train Loss: 0.6706, Train Acc: 0.5216\n",
      "Test Loss: 0.1475, Test Acc: 0.1431\n",
      "Fold1, epoch7:...\n",
      "Train Loss: 0.6391, Train Acc: 0.5402\n",
      "Test Loss: 0.1420, Test Acc: 0.1475\n",
      "Fold1, epoch8:...\n",
      "Train Loss: 0.5680, Train Acc: 0.5781\n",
      "Test Loss: 0.1423, Test Acc: 0.1449\n",
      "Fold1, epoch9:...\n",
      "Train Loss: 0.5458, Train Acc: 0.5819\n",
      "Test Loss: 0.1365, Test Acc: 0.1414\n",
      "Fold1, epoch10:...\n",
      "Train Loss: 0.5135, Train Acc: 0.5946\n",
      "Test Loss: 0.1235, Test Acc: 0.1528\n",
      "Fold1, epoch11:...\n",
      "Train Loss: 0.4952, Train Acc: 0.6022\n",
      "Test Loss: 0.1189, Test Acc: 0.1541\n",
      "Fold1, epoch12:...\n",
      "Train Loss: 0.4644, Train Acc: 0.6165\n",
      "Test Loss: 0.1272, Test Acc: 0.1510\n",
      "Fold1, epoch13:...\n",
      "Train Loss: 0.4216, Train Acc: 0.6268\n",
      "Test Loss: 0.1011, Test Acc: 0.1611\n",
      "Fold1, epoch14:...\n",
      "Train Loss: 0.4138, Train Acc: 0.6386\n",
      "Test Loss: 0.1057, Test Acc: 0.1574\n",
      "Fold1, epoch15:...\n",
      "Train Loss: 0.4177, Train Acc: 0.6340\n",
      "Test Loss: 0.1000, Test Acc: 0.1624\n",
      "Fold1, epoch16:...\n",
      "Train Loss: 0.3840, Train Acc: 0.6489\n",
      "Test Loss: 0.0891, Test Acc: 0.1646\n",
      "Fold1, epoch17:...\n",
      "Train Loss: 0.3779, Train Acc: 0.6537\n",
      "Test Loss: 0.0857, Test Acc: 0.1670\n",
      "Fold1, epoch18:...\n",
      "Train Loss: 0.3443, Train Acc: 0.6594\n",
      "Test Loss: 0.0795, Test Acc: 0.1720\n",
      "Fold1, epoch19:...\n",
      "Train Loss: 0.3336, Train Acc: 0.6669\n",
      "Test Loss: 0.0829, Test Acc: 0.1707\n",
      "Fold1, epoch20:...\n",
      "Train Loss: 0.3138, Train Acc: 0.6772\n",
      "Test Loss: 0.0998, Test Acc: 0.1620\n",
      "1,1\n",
      "Fold2, epoch1:...\n",
      "Train Loss: 0.3384, Train Acc: 0.6719\n",
      "Test Loss: 0.0805, Test Acc: 0.1677\n",
      "Fold2, epoch2:...\n",
      "Train Loss: 0.3304, Train Acc: 0.6724\n",
      "Test Loss: 0.0754, Test Acc: 0.1694\n",
      "Fold2, epoch3:...\n",
      "Train Loss: 0.3269, Train Acc: 0.6715\n",
      "Test Loss: 0.0673, Test Acc: 0.1720\n",
      "Fold2, epoch4:...\n",
      "Train Loss: 0.2990, Train Acc: 0.6892\n",
      "Test Loss: 0.0737, Test Acc: 0.1720\n",
      "Fold2, epoch5:...\n",
      "Train Loss: 0.3020, Train Acc: 0.6866\n",
      "Test Loss: 0.0709, Test Acc: 0.1727\n",
      "Fold2, epoch6:...\n",
      "Train Loss: 0.2859, Train Acc: 0.6925\n",
      "Test Loss: 0.0683, Test Acc: 0.1751\n",
      "Fold2, epoch7:...\n",
      "Train Loss: 0.2679, Train Acc: 0.7011\n",
      "Test Loss: 0.0646, Test Acc: 0.1751\n",
      "Fold2, epoch8:...\n",
      "Train Loss: 0.2755, Train Acc: 0.6921\n",
      "Test Loss: 0.0669, Test Acc: 0.1749\n",
      "Fold2, epoch9:...\n",
      "Train Loss: 0.2604, Train Acc: 0.6980\n",
      "Test Loss: 0.0592, Test Acc: 0.1788\n",
      "Fold2, epoch10:...\n",
      "Train Loss: 0.2538, Train Acc: 0.7052\n",
      "Test Loss: 0.0630, Test Acc: 0.1753\n",
      "Fold2, epoch11:...\n",
      "Train Loss: 0.2691, Train Acc: 0.6956\n",
      "Test Loss: 0.0626, Test Acc: 0.1769\n",
      "Fold2, epoch12:...\n",
      "Train Loss: 0.2293, Train Acc: 0.7153\n",
      "Test Loss: 0.0613, Test Acc: 0.1775\n",
      "Fold2, epoch13:...\n",
      "Train Loss: 0.2410, Train Acc: 0.7063\n",
      "Test Loss: 0.0656, Test Acc: 0.1740\n",
      "Fold2, epoch14:...\n",
      "Train Loss: 0.2426, Train Acc: 0.7052\n",
      "Test Loss: 0.0538, Test Acc: 0.1793\n",
      "Fold2, epoch15:...\n",
      "Train Loss: 0.2327, Train Acc: 0.7136\n",
      "Test Loss: 0.0566, Test Acc: 0.1817\n",
      "Fold2, epoch16:...\n",
      "Train Loss: 0.2203, Train Acc: 0.7199\n",
      "Test Loss: 0.0566, Test Acc: 0.1801\n",
      "Fold2, epoch17:...\n",
      "Train Loss: 0.2102, Train Acc: 0.7204\n",
      "Test Loss: 0.0711, Test Acc: 0.1755\n",
      "Fold2, epoch18:...\n",
      "Train Loss: 0.2282, Train Acc: 0.7129\n",
      "Test Loss: 0.0573, Test Acc: 0.1801\n",
      "Fold2, epoch19:...\n",
      "Train Loss: 0.2032, Train Acc: 0.7226\n",
      "Test Loss: 0.0563, Test Acc: 0.1793\n",
      "Fold2, epoch20:...\n",
      "Train Loss: 0.2056, Train Acc: 0.7190\n",
      "Test Loss: 0.0552, Test Acc: 0.1806\n",
      "2,2\n",
      "Fold3, epoch1:...\n",
      "Train Loss: 0.2233, Train Acc: 0.7177\n",
      "Test Loss: 0.0698, Test Acc: 0.1720\n",
      "Fold3, epoch2:...\n",
      "Train Loss: 0.2335, Train Acc: 0.7103\n",
      "Test Loss: 0.0493, Test Acc: 0.1823\n",
      "Fold3, epoch3:...\n",
      "Train Loss: 0.2283, Train Acc: 0.7136\n",
      "Test Loss: 0.0493, Test Acc: 0.1799\n",
      "Fold3, epoch4:...\n",
      "Train Loss: 0.2049, Train Acc: 0.7217\n",
      "Test Loss: 0.0549, Test Acc: 0.1788\n",
      "Fold3, epoch5:...\n",
      "Train Loss: 0.1983, Train Acc: 0.7243\n",
      "Test Loss: 0.0532, Test Acc: 0.1806\n",
      "Fold3, epoch6:...\n",
      "Train Loss: 0.2132, Train Acc: 0.7193\n",
      "Test Loss: 0.0485, Test Acc: 0.1812\n",
      "Fold3, epoch7:...\n",
      "Train Loss: 0.1905, Train Acc: 0.7291\n",
      "Test Loss: 0.0465, Test Acc: 0.1817\n",
      "Fold3, epoch8:...\n",
      "Train Loss: 0.1753, Train Acc: 0.7353\n",
      "Test Loss: 0.0550, Test Acc: 0.1804\n",
      "Fold3, epoch9:...\n",
      "Train Loss: 0.1810, Train Acc: 0.7318\n",
      "Test Loss: 0.0402, Test Acc: 0.1837\n",
      "Fold3, epoch10:...\n",
      "Train Loss: 0.1794, Train Acc: 0.7304\n",
      "Test Loss: 0.0437, Test Acc: 0.1843\n",
      "Fold3, epoch11:...\n",
      "Train Loss: 0.1873, Train Acc: 0.7307\n",
      "Test Loss: 0.0506, Test Acc: 0.1806\n",
      "Fold3, epoch12:...\n",
      "Train Loss: 0.1590, Train Acc: 0.7383\n",
      "Test Loss: 0.0503, Test Acc: 0.1795\n",
      "Fold3, epoch13:...\n",
      "Train Loss: 0.1623, Train Acc: 0.7421\n",
      "Test Loss: 0.0485, Test Acc: 0.1812\n",
      "Fold3, epoch14:...\n",
      "Train Loss: 0.1858, Train Acc: 0.7280\n",
      "Test Loss: 0.0485, Test Acc: 0.1817\n",
      "Fold3, epoch15:...\n",
      "Train Loss: 0.1524, Train Acc: 0.7445\n",
      "Test Loss: 0.0521, Test Acc: 0.1801\n",
      "Fold3, epoch16:...\n",
      "Train Loss: 0.1583, Train Acc: 0.7388\n",
      "Test Loss: 0.0551, Test Acc: 0.1810\n",
      "Fold3, epoch17:...\n",
      "Train Loss: 0.1546, Train Acc: 0.7412\n",
      "Test Loss: 0.0451, Test Acc: 0.1843\n",
      "Fold3, epoch18:...\n",
      "Train Loss: 0.1463, Train Acc: 0.7473\n",
      "Test Loss: 0.0413, Test Acc: 0.1850\n",
      "Fold3, epoch19:...\n",
      "Train Loss: 0.1429, Train Acc: 0.7458\n",
      "Test Loss: 0.0467, Test Acc: 0.1828\n",
      "Fold3, epoch20:...\n",
      "Train Loss: 0.1536, Train Acc: 0.7416\n",
      "Test Loss: 0.0401, Test Acc: 0.1854\n",
      "3,3\n",
      "Fold4, epoch1:...\n",
      "Train Loss: 0.1639, Train Acc: 0.7361\n",
      "Test Loss: 0.0351, Test Acc: 0.1869\n",
      "Fold4, epoch2:...\n",
      "Train Loss: 0.1534, Train Acc: 0.7423\n",
      "Test Loss: 0.0410, Test Acc: 0.1839\n",
      "Fold4, epoch3:...\n",
      "Train Loss: 0.1636, Train Acc: 0.7429\n",
      "Test Loss: 0.0346, Test Acc: 0.1858\n",
      "Fold4, epoch4:...\n",
      "Train Loss: 0.1344, Train Acc: 0.7497\n",
      "Test Loss: 0.0276, Test Acc: 0.1891\n",
      "Fold4, epoch5:...\n",
      "Train Loss: 0.1276, Train Acc: 0.7497\n",
      "Test Loss: 0.0377, Test Acc: 0.1852\n",
      "Fold4, epoch6:...\n",
      "Train Loss: 0.1231, Train Acc: 0.7495\n",
      "Test Loss: 0.0310, Test Acc: 0.1885\n",
      "Fold4, epoch7:...\n",
      "Train Loss: 0.1304, Train Acc: 0.7478\n",
      "Test Loss: 0.0384, Test Acc: 0.1854\n",
      "Fold4, epoch8:...\n",
      "Train Loss: 0.1275, Train Acc: 0.7530\n",
      "Test Loss: 0.0350, Test Acc: 0.1867\n",
      "Fold4, epoch9:...\n",
      "Train Loss: 0.1201, Train Acc: 0.7554\n",
      "Test Loss: 0.0382, Test Acc: 0.1830\n",
      "Fold4, epoch10:...\n",
      "Train Loss: 0.1410, Train Acc: 0.7486\n",
      "Test Loss: 0.0423, Test Acc: 0.1826\n",
      "Fold4, epoch11:...\n",
      "Train Loss: 0.1141, Train Acc: 0.7572\n",
      "Test Loss: 0.0356, Test Acc: 0.1876\n",
      "Fold4, epoch12:...\n",
      "Train Loss: 0.1057, Train Acc: 0.7605\n",
      "Test Loss: 0.0330, Test Acc: 0.1883\n",
      "Fold4, epoch13:...\n",
      "Train Loss: 0.1084, Train Acc: 0.7574\n",
      "Test Loss: 0.0516, Test Acc: 0.1808\n",
      "Fold4, epoch14:...\n",
      "Train Loss: 0.1162, Train Acc: 0.7574\n",
      "Test Loss: 0.0460, Test Acc: 0.1819\n",
      "Fold4, epoch15:...\n",
      "Train Loss: 0.1192, Train Acc: 0.7550\n",
      "Test Loss: 0.0322, Test Acc: 0.1883\n",
      "Fold4, epoch16:...\n",
      "Train Loss: 0.1100, Train Acc: 0.7587\n",
      "Test Loss: 0.0480, Test Acc: 0.1819\n",
      "Fold4, epoch17:...\n",
      "Train Loss: 0.1320, Train Acc: 0.7530\n",
      "Test Loss: 0.0360, Test Acc: 0.1856\n",
      "Fold4, epoch18:...\n",
      "Train Loss: 0.0991, Train Acc: 0.7627\n",
      "Test Loss: 0.0331, Test Acc: 0.1856\n",
      "Fold4, epoch19:...\n",
      "Train Loss: 0.0915, Train Acc: 0.7664\n",
      "Test Loss: 0.0367, Test Acc: 0.1856\n",
      "Fold4, epoch20:...\n",
      "Train Loss: 0.0922, Train Acc: 0.7638\n",
      "Test Loss: 0.0378, Test Acc: 0.1863\n",
      "4,4\n",
      "Fold5, epoch1:...\n",
      "Train Loss: 0.1521, Train Acc: 0.7425\n",
      "Test Loss: 0.0320, Test Acc: 0.1891\n",
      "Fold5, epoch2:...\n",
      "Train Loss: 0.1396, Train Acc: 0.7467\n",
      "Test Loss: 0.0377, Test Acc: 0.1832\n",
      "Fold5, epoch3:...\n",
      "Train Loss: 0.1362, Train Acc: 0.7467\n",
      "Test Loss: 0.0317, Test Acc: 0.1885\n",
      "Fold5, epoch4:...\n",
      "Train Loss: 0.1402, Train Acc: 0.7469\n",
      "Test Loss: 0.0287, Test Acc: 0.1898\n",
      "Fold5, epoch5:...\n",
      "Train Loss: 0.1258, Train Acc: 0.7486\n",
      "Test Loss: 0.0293, Test Acc: 0.1893\n",
      "Fold5, epoch6:...\n",
      "Train Loss: 0.1261, Train Acc: 0.7506\n",
      "Test Loss: 0.0423, Test Acc: 0.1845\n",
      "Fold5, epoch7:...\n",
      "Train Loss: 0.1239, Train Acc: 0.7565\n",
      "Test Loss: 0.0312, Test Acc: 0.1880\n",
      "Fold5, epoch8:...\n",
      "Train Loss: 0.1281, Train Acc: 0.7515\n",
      "Test Loss: 0.0311, Test Acc: 0.1878\n",
      "Fold5, epoch9:...\n",
      "Train Loss: 0.1204, Train Acc: 0.7561\n",
      "Test Loss: 0.0341, Test Acc: 0.1869\n",
      "Fold5, epoch10:...\n",
      "Train Loss: 0.0955, Train Acc: 0.7648\n",
      "Test Loss: 0.0336, Test Acc: 0.1878\n",
      "Fold5, epoch11:...\n",
      "Train Loss: 0.1096, Train Acc: 0.7583\n",
      "Test Loss: 0.0357, Test Acc: 0.1883\n",
      "Fold5, epoch12:...\n",
      "Train Loss: 0.1109, Train Acc: 0.7605\n",
      "Test Loss: 0.0385, Test Acc: 0.1858\n",
      "Fold5, epoch13:...\n",
      "Train Loss: 0.0960, Train Acc: 0.7616\n",
      "Test Loss: 0.0368, Test Acc: 0.1865\n",
      "Fold5, epoch14:...\n",
      "Train Loss: 0.0910, Train Acc: 0.7657\n",
      "Test Loss: 0.0389, Test Acc: 0.1841\n",
      "Fold5, epoch15:...\n",
      "Train Loss: 0.1023, Train Acc: 0.7651\n",
      "Test Loss: 0.0363, Test Acc: 0.1885\n",
      "Fold5, epoch16:...\n",
      "Train Loss: 0.0863, Train Acc: 0.7677\n",
      "Test Loss: 0.0374, Test Acc: 0.1874\n",
      "Fold5, epoch17:...\n",
      "Train Loss: 0.1034, Train Acc: 0.7600\n",
      "Test Loss: 0.0532, Test Acc: 0.1826\n",
      "Fold5, epoch18:...\n",
      "Train Loss: 0.0924, Train Acc: 0.7627\n",
      "Test Loss: 0.0546, Test Acc: 0.1819\n",
      "Fold5, epoch19:...\n",
      "Train Loss: 0.1002, Train Acc: 0.7638\n",
      "Test Loss: 0.0399, Test Acc: 0.1869\n",
      "Fold5, epoch20:...\n",
      "Train Loss: 0.1013, Train Acc: 0.7611\n",
      "Test Loss: 0.0448, Test Acc: 0.1847\n",
      "5,5\n",
      "Average Validation Accuracy: 0.1747\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "model = BiLSTMModel(input_size, hidden_size, num_layers, num_directions, num_classses,dropout_prob=0.1)\n",
    "model = model.to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "loss_func = nn.CrossEntropyLoss() # 与categorical crossEntropyLoss不同，CrossEntropyLoss期望输出是一个类索引，而不是独热编码\n",
    "model,train_loss,train_acc,CV_loss,CV_acc = train(model, train_loader,optimizer, loss_func, epochs,k_fold=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 1.7972, Test Acc: 0.5049\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1.7971768137030817, 0.504930966469428)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test(model,test_loader,loss_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 1.9838184990591317, Test Acc: 0.4714003944773176\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1.9838184990591317, 0.4714003944773176)"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test(model,test_loader,loss_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch-cpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
